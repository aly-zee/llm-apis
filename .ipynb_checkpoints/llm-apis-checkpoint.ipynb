{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c514f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case: Using gpt to translate corporate jargon to a newbie. GPT will explain the meaning of a specific corporate jargon to someone that is new to working in a corporate enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ebaac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = open(\"key.txt\",\"r\").read().strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbe3d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# load and set our key\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b88fc8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history=[{\"role\": \"system\", \"content\": \"You are a useful translation assistant.\"}]\n",
    "\n",
    "def gpt_response(inp, message_history):\n",
    "    # We save the user's input\n",
    "    message_history.append({\"role\": \"user\", \"content\": f\"{inp}\"})\n",
    "\n",
    "    # Generate a response from the chatbot model\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=message_history\n",
    "    )\n",
    "\n",
    "    # We save the assistant response\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": f\"{completion.choices[0].message.content}\"})\n",
    "\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bcf506b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> what does boil the ocean mean\n",
      "The phrase \"boil the ocean\" is an idiomatic expression that means attempting to do something impossible or extremely challenging, often in a wasteful or unnecessarily complex manner. It implies an unrealistic or overly ambitious goal that is unlikely to be achieved.\n",
      "> what does brain dump mean\n",
      "\"Brain dump\" is a term used to describe the act of taking all the knowledge or information stored in one's brain and transferring it onto paper, a document, or a digital format. It can also refer to a situation where someone quickly shares all the information they have on a particular topic without much organization or structure.\n",
      "> what about break down silos\n",
      "\"Breaking down silos\" is a business term that refers to breaking down barriers or communication blocks between different departments, teams, or individuals within an organization. Silos can hinder cooperation, collaboration, and information sharing, leading to inefficiencies and missed opportunities. Breaking down silos involves promoting cross-departmental collaboration, sharing information and resources, and fostering a more cohesive and integrated work environment.\n",
      "> above my paygrade\n",
      "The phrase \"above my paygrade\" is often used to indicate that a task, decision, or topic is beyond the scope of one's authority, expertise, or responsibility within an organization. Essentially, it means that the matter in question is better suited for someone of a higher position or rank.\n",
      "> drill down\n",
      "\"Drill down\" is a term used to describe the process of navigating through layers of information or data to get to the specific details or root causes of a situation. It involves delving deeper into a topic or dataset to gain a more in-depth understanding of the subject matter. By drilling down, individuals can uncover insights, identify patterns, and make informed decisions based on a thorough analysis.\n",
      "> move the goal post\n",
      "\"Move the goalposts\" is an idiom that means to change the rules, requirements, or criteria of a situation or agreement in a way that makes it difficult for someone to achieve success or meet the original expectations. It implies shifting the target or standards after the fact, often unfairly or unexpectedly, to disadvantage the other party.\n",
      "> \n",
      "If you have any more questions or need further assistance, feel free to ask! I'm here to help.\n",
      "> \n",
      "If you need assistance with anything else in the future, don't hesitate to reach out. Have a great day!\n",
      "> \n",
      "Goodbye! If you have any questions in the future, feel free to ask. Have a wonderful day!\n",
      "> \n",
      "Goodbye!\n",
      "> \n",
      "Goodbye!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     message_history \u001b[38;5;241m=\u001b[39m gpt_response(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m> \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, message_history) \u001b[38;5;66;03m# Lets ask by input different prompts\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(message_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    message_history = gpt_response(input(\"> \"), message_history) # Lets ask by input different prompts\n",
    "    print(message_history[-1][\"content\"]) # Print the last response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43761bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d70e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment analysis: Decide if the corporate jargon said is positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f154f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_response(inp, message_history, **params):\n",
    "    # We save the user's input\n",
    "    message_history.append({\"role\": \"user\", \"content\": f\"{inp}\"})\n",
    "\n",
    "    # Generate a response from the chatbot model\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": message_history,\n",
    "        **params  # Include additional parameters\n",
    "    }\n",
    "\n",
    "    completion = openai.ChatCompletion.create(**completion_params) # Include additional parameters\n",
    "\n",
    "    # We save the assistant response\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": f\"{completion.choices[0].message.content}\"})\n",
    "\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fc92407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corporate Jargon: \n",
      "> voluntold\n",
      "Neutral\n",
      "> put out a fire\n",
      "Neutral\n",
      "> imporve performance\n",
      "Positive\n",
      "> lacking motivation\n",
      "Negative\n",
      "> reinventing the wheel\n",
      "Neutral\n",
      "> per my last email\n",
      "Neutral\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorporate Jargon: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m----> 9\u001b[0m     message_history \u001b[38;5;241m=\u001b[39m gpt_response(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m> \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     10\u001b[0m                                    message_history,\n\u001b[0;32m     11\u001b[0m                                    temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     12\u001b[0m                                    max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m,\n\u001b[0;32m     13\u001b[0m                                    frequency_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(message_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "message_history=[{\"role\": \"system\", \"content\": \"Decide whether jargon sentiment is positive, neutral, or negative.\"},\n",
    "             {\"role\": \"user\", \"content\": \"Jargon: \\\"You're crushing it!\\\"\\nSentiment:\"},\n",
    "              {\"role\": \"assistant\", \"content\": \"Positive\"}\n",
    "  ]\n",
    "\n",
    "print(\"Corporate Jargon: \")\n",
    "\n",
    "while(True):\n",
    "    message_history = gpt_response(input(\"> \"),\n",
    "                                   message_history,\n",
    "                                   temperature=0,\n",
    "                                   max_tokens=60,\n",
    "                                   frequency_penalty=0.5)\n",
    "    print(message_history[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15782683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis: Continued from above but changed the content to a negative corporate jargon to see results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2649011a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corporate Jargon: \n",
      "> per my last email\n",
      "Negative\n",
      "> put out a fire\n",
      "Neutral\n",
      "> low hanging fruit\n",
      "Neutral\n",
      "> Herding cats\n",
      "Negative\n",
      "> \n",
      "\n",
      "> \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorporate Jargon: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m----> 9\u001b[0m     message_history \u001b[38;5;241m=\u001b[39m gpt_response(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m> \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     10\u001b[0m                                    message_history,\n\u001b[0;32m     11\u001b[0m                                    temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     12\u001b[0m                                    max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m,\n\u001b[0;32m     13\u001b[0m                                    frequency_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(message_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "message_history=[{\"role\": \"system\", \"content\": \"Decide whether jargon sentiment is positive, neutral, or negative.\"},\n",
    "             {\"role\": \"user\", \"content\": \"Jargon: \\\"Per my last email!\\\"\\nSentiment:\"},\n",
    "              {\"role\": \"assistant\", \"content\": \"Negative\"}\n",
    "  ]\n",
    "\n",
    "print(\"Corporate Jargon: \")\n",
    "\n",
    "while(True):\n",
    "    message_history = gpt_response(input(\"> \"),\n",
    "                                   message_history,\n",
    "                                   temperature=0,\n",
    "                                   max_tokens=60,\n",
    "                                   frequency_penalty=0.5)\n",
    "    print(message_history[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a0aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "108c76f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_response(inp, message_history, **params):\n",
    "    # We save the user's input\n",
    "    message_history.append({\"role\": \"user\", \"content\": f\"{inp}\"})\n",
    "\n",
    "    # Generate a response from the chatbot model\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": message_history,\n",
    "        **params  # Include additional parameters\n",
    "    }\n",
    "\n",
    "    completion = openai.ChatCompletion.create(**completion_params) # Include additional parameters\n",
    "\n",
    "    # We save the assistant response\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": f\"{completion.choices[0].message.content}\"})\n",
    "\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79f41cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I regret to inform you that I will be unable to attend the meeting. \n",
      "2. I'm sorry, I will not be able to be present at the meeting. \n",
      "3. I have to to be to be to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to\n",
      "\n",
      "1. Unfortunately, I have a conflicting appointment and won't be able to make it to the meeting.\n",
      "2. Due to prior commitments, I regret that I will not be able to attend the meeting.\n",
      "3. Regrettably, unforeseen circumstances have arisen which prevent me from being present at the meeting.\n",
      "4. Apologies for any inconvenience caused; however, I am unable to participate in the meeting as scheduled.\n",
      "5. As much as I would like to attend, something has come up and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message_history=[]\n",
    "\n",
    "for i in [-2,2]:\n",
    "    message_history = gpt_response(\"generate 5 ways to say you wont be able to attend the meeting\",\n",
    "                                   message_history,\n",
    "                                   max_tokens=100,\n",
    "                                   presence_penalty=i,\n",
    "                                   frequency_penalty=i)\n",
    "    print(message_history[-1][\"content\"]+\"\\n\")# Print the last response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891fe279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
